---
layout: post
title: "Assessing Accuracy of Google Maps Cycling Estimates"
excerpt: "I compare cycling times from real trips of Bay Area Bike Share users to Google Maps cycling time estimates."
categories: [R, Viz]
comments: true
---

I like riding my bike for transportation whenever possible, and I often use Google Maps to find a route that is both safe and direct. However, their cycling directions sometimes seem a bit off, and I have often wondered how accurate the time estimates actually are for the average cyclist. This skepticism has been echoed by [others](http://www.betterbybicycle.com/2014/09/how-accurate-are-google-maps-cycling.html) as well.

Recently, I have been experimenting with data from the Bay Area Bike Share (BABS) system -- a network of shared bikes docked at stations scattered around the Bay Area. In this analysis, I compare actual trip times from the BABS to Google Maps time estimates to investigate their accuracy.

```{r echo=FALSE}
PATH <- "~/Documents/Projects/BikeShare/"
```

```{r warning=FALSE, message=FALSE, echo=FALSE}
library(dplyr)
library(readr)
library(stringr)
library(knitr)
library(gridExtra)
library(ggplot2)
library(ggthemes)
source(paste0(PATH, "R/helpers.R"))
source(paste0(PATH, "R/google-api-trip-estimates.R"))
```

## Data

### BABS

The BABS makes trip data publically available [here](http://www.bayareabikeshare.com/open-data). It consists of individual records for 983,648 trips made between 8/2013 and 8/2016 complete with information like origin, destination, duration, start time, and more.

```{r warning=FALSE, message=FALSE, echo=FALSE}
trip_data <- getAllTripData()
```

There are five different cities served by the system: San Francisco, Redwood City, Palo Alto, Mountain View, and San Jose. While it is possible to take a bike between cities, I filter those trips out because of their rarity and the likelihood that a trip from San Francisco to Palo Alto, for example, includes a stint on the Caltrain.

Below is a histogram of the actual duration of these trips. It is cut off at 30 minutes, but there are some trips that take significantly longer. These outliers are dealt with later. The distribution is skewed off to the right as we would expect since a cyclist can take an arbitrarily long amount of time to complete their trip.

```{r dist-trip-length, warning=FALSE, message=FALSE, echo=FALSE}
ggplot(trip_data, aes(actual_duration/60)) +
  geom_histogram(fill = "grey") +
  lims(x = c(0, 30)) +
  theme_fivethirtyeight() + 
  theme(axis.title = element_text()) +
  labs(title = "Distribution of Actual Trip Length",
       x = "Duration (minutes)",
       y = "Count of Trips")
```

### Google Maps Estimates

Through the Google Maps Directions API, I pull cycling estimates for every pair of stations in the same city. See [here](https://github.com/llefebure/bike-sharing/blob/master/R/google-api-trip-estimates.R) for the full code that generates those numbers. In addition, some stations changed location, so certain routes affected by this have multiple estimates with corresponding date ranges.

```{r warning=FALSE, message=FALSE, echo=FALSE}
gmaps_estimates <- getGoogleMapsCyclingEstimates()
gmaps_estimates <- gmaps_estimates %>%
  filter(origin_id != destination_id)
```

The relationship between the cycling distance (in meters) of a trip and its expected duration (in seconds) as reported by Google Maps is plotted below for each route. There is clearly a strong linear trend with some heteroskedasticity. As trips increase in distance, the variance of their expected duration increases. Trips range in distance from about 50 meters to 6,000 meters and in duration from about 10 seconds to 23 minutes.

```{r gmaps-routes, warning=FALSE, message=FALSE, echo=FALSE}
ggplot(gmaps_estimates, aes(distance/1000, duration/60, color = landmark)) + 
  geom_point(size=.4, alpha=.8) + 
  theme_fivethirtyeight() +
  theme(axis.title = element_text()) +
  scale_color_ptol("") + 
  labs(x = "Distance (km)",
       y = "Duration (minutes)",
       title = "Google Maps Route Estimates")
```

From this data, I infer the expected average cycling speed for each route by scaling the ratio of duration and time. This distribution is shown below. The average expected speed of a route is between 8 and 9 miles per hour.

```{r gmaps-avg-speed, warning=FALSE, message=FALSE, echo=FALSE}
ggplot(gmaps_estimates, aes((distance/duration)/1000*3600/1.609)) + 
  geom_histogram(fill = "grey") + 
  theme_fivethirtyeight() +
  theme(axis.title = element_text()) +
  labs(x = "Speed (mph)",
       y = "Count of Routes",
       title = "Average Expected Speed by Route")
```

```{r warning=FALSE, message=FALSE, echo=FALSE}
trip_data_with_est <- inner_join(trip_data, gmaps_estimates) %>%
  mutate(origin_date_match = as.Date(time) >= start_date_origin & 
           as.Date(time) <= end_date_origin,
         destination_date_match = as.Date(time) >= start_date_destination & 
           as.Date(time) <= end_date_destination) %>%
  filter(is.na(origin_date_match) | origin_date_match == TRUE, 
         is.na(destination_date_match) | destination_date_match == TRUE) %>%
  group_by(trip_id) %>%
  arrange(origin_date_match, destination_date_match) %>%
  filter(row_number() == 1) %>%
  ungroup() %>%
  rename(gmaps_duration = duration, gmaps_distance = distance) %>%
  mutate(duration_diff = actual_duration - gmaps_duration) %>%
  select(origin_id, destination_id, origin_station, destination_station, origin_latlong, 
         destination_latlong, subscription, actual_duration, gmaps_duration, duration_diff,
         gmaps_distance, year, month, day, weekday, hour, minute, landmark)
```

## Removing Major Outliers

My ultimate goal is to compare actual trip times with Google Maps estimates, so I need to filter out trips that were not continuous point to point journeys. For example, it is possible that a tourist stops several times to take photos before reaching their destination. Trips such as these could very easily skew the analysis. A quick look at the distribution of trip times (in seconds) reveals at least one obvious outlier.

```{r warning=FALSE, message=FALSE, echo=FALSE}
trip_time_dist <- data.frame(t(as.matrix(summary(trip_data_with_est$actual_duration))))
colnames(trip_time_dist) <- c("Min", "First Quartile", "Median", "Mean", "Third Quartile", "Max")
kable(trip_time_dist)
```

Clearly nobody made a continuous trip of over 17,000,000 seconds, which equates to approximately 200 days. However, finding the not so obvious discontinuous trips is more challenging. The BABS imposes overage charges on any trip that lasts longer than 30 minutes, so the system is setup to discourage those longer, discontinuous journeys. As a first step, I will filter out all trips longer than 30 minutes.

```{r warning=FALSE, message=FALSE, echo=FALSE}
trip_data_with_est <- trip_data_with_est %>%
  filter(actual_duration <= 1800)
```

There is an additional piece of important information attached to each trip -- the subscriber type. This tells us more about the rider. Subscribers are those with annual or 30 day memberships, and customers are those with 24 hour or 3 day passes. I expect that subscribers are those that use the system for mostly commuting and transportation (the trips we are interested in), while customers can include tourists who use the system for exploring the area. Plotted below are the distributions of the difference between a trip's actual time and its estimated time by subscriber type. As expected, customers ride slower than subscribers. I filter out these trips as well.

```{r duration-diff-segmented, warning=FALSE, message=FALSE, echo=FALSE}
ggplot(trip_data_with_est, aes(duration_diff/60, ..density.., color=subscription)) + 
  geom_freqpoly() +
  theme_fivethirtyeight() +
  scale_color_fivethirtyeight("") +
  theme(axis.title = element_text()) +
  labs(title = "Comparison of Trip Duration with\nGoogle Maps Estimate",
       x = "Difference",
       y = "Frequency")
```

```{r warning=FALSE, message=FALSE, echo=FALSE}
trip_data_with_est <- trip_data_with_est %>%
  filter(subscription == "Subscriber")
```

## Analysis

The remaining dataset now has all trips shorter than 30 minutes made by subscribers. In the analysis that follows, I work with this dataset and explore descriptive statistics, particularly the median. Because the distribution of trip times is heavily skewed to the right and some outliers inevitably remain, the median is a more meaningful gauge of the "average" or "typical" trip than the mean itself.

The distribution of the difference (in seconds) between the actual and estimated time of each trip is shown below. The median is just 21 seconds, suggesting that the Google Maps estimates are quite good. These 21 seconds could simply be the overhead amount of time that it takes to check the bike in and out on either end of the trip. Despite the long right tail, the quartiles are nearly perfectly symmetric as well -- the middle 50% of trips were within just over one minute of the median.

```{r warning=FALSE, message=FALSE, echo=FALSE}
trip_diff <- data.frame(t(as.matrix(summary(trip_data_with_est$duration_diff))))
colnames(trip_diff) <- c("Min", "First Quartile", "Median", "Mean", "Third Quartile", "Max")
kable(trip_diff)
```

```{r duration-diff, warning=FALSE, message=FALSE, echo=FALSE}
ggplot(trip_data_with_est, aes(duration_diff)) + 
  geom_histogram(fill = "grey") + 
  theme_fivethirtyeight() +
  theme(axis.title = element_text()) +
  labs(title = "Comparison of Trip Duration with\nGoogle Maps Estimate",
       x = "Actual Duration Minus Google Maps Estimate",
       y = "Count of Trips") +
  lims(x = c(-400, 1000))
```

However, trips vary in length significantly. A one minute difference for a five minute trip is much different than a one minute difference for a twenty minute trip. Instead of looking at the difference between the actual and estimated times, I look at this difference scaled by the expected time below. Some outliers were missed, but the distribution is still remarkably symmetric. The median trip is just 5% longer than estimated.

```{r warning=FALSE, message=FALSE, echo=FALSE}
trip_data_with_est <- trip_data_with_est %>%
  mutate(duration_diff_prop = duration_diff/gmaps_duration)
```

```{r warning=FALSE, message=FALSE, echo=FALSE}
trip_diff_prop <- data.frame(t(as.matrix(summary(trip_data_with_est$duration_diff_prop))))
colnames(trip_diff_prop) <- c("Min", "First Quartile", "Median", "Mean", "Third Quartile", "Max")
kable(trip_diff_prop)
```

```{r duration-diff-prop, warning=FALSE, message=FALSE, echo=FALSE}
ggplot(trip_data_with_est, aes(x=duration_diff_prop)) + 
  geom_histogram(fill = "grey") + 
  theme_fivethirtyeight() +
  theme(axis.title = element_text()) +
  labs(title = "Comparison of Trip Duration with\nGoogle Maps Estimate",
       x = "Proportion Difference from Google Maps Estimate",
       y = "Count of Trips") +
  lims(x = c(-1, 2))
```

Next, I segment the trips by city. Note that the vast majority of trips were in San Francisco and that the two cities with the highest usage, SF and San Jose, match the Google Maps estimates most closely. In the other cities, the estimates don't match with the actual trip times very well, however the service in these cities was eventually discontinued suggesting that usage never really took off there.

```{r warning=FALSE, message=FALSE, echo=FALSE}
by_city_stats <- data.frame(trip_data_with_est %>% 
                              group_by(landmark) %>% 
                              summarize(cnt = n(), 
                                        median = median(duration_diff_prop),
                                        median_s = median(duration_diff)) %>%
                              mutate(prop = cnt/sum(cnt)))
colnames(by_city_stats) <- c("City", "Count of Trips", "Median Difference From Estimated Duration (proportion)", "Median Difference From Estimated Duration (s)", "Proportion of Total")
kable(by_city_stats[,c(1,2,5,3,4)])
```

```{r segmented-by-city, warning=FALSE, message=FALSE, echo=FALSE}
ggplot(trip_data_with_est, aes(factor(landmark), duration_diff_prop)) + 
  geom_boxplot() + 
  theme_fivethirtyeight() +
  lims(y = c(-1,2)) +
  labs(x = NULL, y = "Proportion") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text.y = element_text(),
        axis.title = element_text()) +
  labs(title = "Proportion Difference From Estimate")
```

Finally, I segment the trips by their expected distance to determine whether the accuracy of estimates differs between shorter and longer trips. The 0%-25% bucket contains, for example, all trips for which the Google Maps estimate was among the shortest quarter. The plots below show that the estimates for shorter trips are slightly too fast and those for longer trips are a bit slow. Two factors that could account for this difference are:

* Only better and faster cyclists attempt longer journeys on the bike, making those estimates seem too slow.
* The overhead amount of time that it takes to check out a bike, possibly adjust the seat, and check in the bike at the other end impacts shorter journeys more, making those estimates seem too fast.

```{r warning=FALSE, message=FALSE, echo=FALSE}
qs <- as.numeric(quantile(trip_data_with_est$gmaps_duration, probs = c(.25, .5, .75)))
getQuartileBucket <- function(d) {
  if (d <= qs[1]) "0%-25%"
  else if (d <= qs[2]) "25%-50%"
  else if (d <= qs[3]) "50%-75%"
  else "75%-100%"
}
trip_data_with_est$quartile_bucket <- sapply(trip_data_with_est$gmaps_duration, getQuartileBucket)
```

```{r warning=FALSE, message=FALSE, echo=FALSE}
by_length_stats <- data.frame(trip_data_with_est %>% 
                                group_by(quartile_bucket) %>% 
                                summarize(median = median(duration_diff_prop),
                                          median_s = median(duration_diff)))
colnames(by_length_stats) <- c("Quartile Bucket", "Median Difference From Estimated Duration (proportion)", "Median Difference From Estimated Duration (s)")
kable(by_length_stats)
```

```{r segmented-by-distance, warning=FALSE, message=FALSE, echo=FALSE}
ggplot(trip_data_with_est, aes(factor(quartile_bucket), duration_diff_prop)) + 
  geom_boxplot() + 
  theme_fivethirtyeight() +
  lims(y = c(-1,2)) +
  labs(x = NULL, y = "Proportion") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text.y = element_text(),
        axis.title = element_text()) +
  labs(title = "Proportion Difference From Estimate")
```

## Conclusion

Google Maps estimates seem to be very accurate for the median rider in San Francisco. When segmented out by trip distance, the estimates were slightly too fast for shorter routes and too slow for longer routes. This analysis gives some quantitative intuition about urban cycling estimates in the Bay Area, but I cannot make broader conclusions because the data was restricted to subscribers in the BABS on very specific and short, urban routes. It is quite possible that Google even uses this data to calibrate their models. They probably want their estimates to be as accurate as possible in their backyard!

One factor that could have skewed these results is missed outliers (trips that were not continuous point to point journeys). As discussed previously, I removed some of these trips because of their unrealistically long length, but there are definitely some that were missed. It is possible that there exists outliers on the other end of the spectrum too -- trips that were unrealistically short. The BABS employs rebalancers that drive around and redistribute bikes to different stations in the system. I could not figure out whether these types of "trips" are included in the dataset or not, but their presence would certainly be probelmatic. Despite all of these issues, I think that the median still gives a realistic picture because it is relatively resistant to outliers.

Now when I use Google Maps for cycling directions to get somewhere, I have some quantitative intuition as to their time accuracy. I will put aside ego, honestly assess how much better (or worse) than the median rider I think I am on that day, and extrapolate accordingly!